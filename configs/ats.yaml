timesteps: [0,250]
# be consistent with UBP
blur_kernel_size: 51 
system_g: 3
# determine the latent space dimension
downsample_dim: 768
name: "${exp_setting}_ubp_${brain_backbone}_${vision_backbone}_${adaptor_backbone}"


models:
  brain:
    target: "base.eeg_backbone.${brain_backbone}"
    params:
        c_num: 17
        z_dim: ${downsample_dim}
        timesteps: ${timesteps}
  shrink_adapter:
    target: "base.ShrinkAdaptor.${adaptor_backbone}"  # chose the adaptor backbone
    params:
      input_dim: ${z_dim}
      output_dim: ${downsample_dim}

data:
  data_dir: /xxx/Preprocessed_data_250Hz_whiten # Update this path to your actual data directory
  selected_ch: ['P7', 'P5', 'P3', 'P1','Pz', 'P2', 'P4', 'P6', 'P8', 'PO7', 'PO3', 'POz', 'PO4', 'PO8','O1', 'Oz', 'O2']
  model_type: ${vision_backbone}
  train_batch_size: 1024
  val_batch_size: 200
  test_batch_size: 200
  uncertainty_aware: True
  # be consistent with UBP
  blur_type:
    target: base.inpating_data.FoveaBlur
    params:
          h: 224
          w: 224
          blur_kernel_size: ${blur_kernel_size}
          curve_type: exp
          system_g:  ${system_g} 
  matai_avg: True
  matai_alpha: 0
  train_avg: True
  test_avg: True
  timesteps: ${timesteps}

train:
  epoch: ${epoch}
  optimizer: AdamW
  lr: ${lr}

save_dir: 'shrinkadapter_1/4_downdim_768'
